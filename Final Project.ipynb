{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation Whirling Dervish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In recent years, the popularity of rock climbing has surged [citation 1]. As more novices take to the climbing wall, more people are becoming interested in viewing rock climbing on a professional level. Each year, hundreds of professional athletes compete in the IFSC Climbing World Cup. A wide array of countries, ages, and body types are represented as well as varying years of experience. Competition climbing also has its nuances, and an athlete who excels outdoors might not demonstrate the same level of performance in the climbing gym.\n",
    "\n",
    "Many professional sports, such as baseball and basketball, have had extensive analysis performed on them with the goals of identifying factors for success and forecasting winning teams. The detailed collection of data and the clear objective of sporting events make such problems well suited for forecasting using data analytics and machine learning techniques. Similar research in competition climbing has analyzed factors that impact performance, although their methods rely either on interviews with domain experts or experiments in closed, controlled settings [citation 2, citation 3]. In this project, we analyze data on athletes and events in the IFSC Climbing World Cup. Our goal is to forecast the climbing world champions in the next 5 years. Throughout this paper, we focus on our results and analysis for the Men’s Lead category. At the end, we run our model to get predictions in the other competition categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Data\n",
    "\n",
    "Our data comes from the International Federation of Sport Climbing (IFSC). It was collected from the website digitalrock.de, a website maintained by Digital ROCK, a contractor for IFSC. Because the organization is contracted by the IFSC, we feel the data is trustworthy. This dataset is a good choice for our problem, because we are able to use the attributes of the athletes, and the trajectory of their career to make our predictions. We expect our analysis to reveal which of these features are most important in determining the performance of an athlete.\n",
    "\n",
    "It is important to note that although in the end we want to forecast athletes’ rank, the way the IFSC calculates the rank is somewhat ill-conditioned for our methods. This is because the rank is calculated based on an athletes’ total score across all World Cup events, even if the athlete didn’t attend several of them (which is quite common). This means that an athlete who places first place in all three events they attend could be ranked below an athlete who placed fifth place in six events. We feel that a more consistent metric for the performance of an athlete is the average points scored per event. Throughout our modeling efforts we use the average points per event as the target value, and we compare it to a ranking in the final step.\n",
    "\n",
    "It should also be noted that the difference in the number of points awarded at the next highest rank increases drastically as the rank approaches 1st place, as shown below.\n",
    "\n",
    "### Data Scraping\n",
    "\n",
    "We used the Python packages `selenium` and `bs4` to do our data collection. We scraped 3 total datasets which we then merged together and used to form features during the Data Cleaning and Feature Engineering process, outlined later.\n",
    "\n",
    "__Athlete DataFrame:__\n",
    "\n",
    "Because of the way the website that we used to get our data was organized, to get athlete data we ended up running through about 200000 athlete ids (not all of which have athletes assigned) overnight with 50 separate workers running asynchronously to pull the data from their profile pages, found at `https://www.digitalrock.de/egroupware/ranking/sitemgr/digitalrock/pstambl.html#person={athlete_id}`. This dataset contains approximately 19,000 rows.\n",
    "\n",
    "|    id | first_name   | last_name   | country   |   age |   birth_year | height   | weight   |\n",
    "|------:|:-------------|:------------|:----------|------:|-------------:|:---------|:---------|\n",
    "| 51001 | Moritz       | Simmet      | GER       |    21 |         1999 | nan      | nan      |\n",
    "| 51002 | Martin       | Eisensteger | GER       |    22 |         1997 | nan      | nan      |\n",
    "| 51004 | Vinzenz      | Kreuzer     | GER       |    24 |         1996 | nan      | nan      |\n",
    "| 51007 | Lukas        | Achermann   | SUI       |    17 |         2003 | 176 cm   | 66 kg    |\n",
    "| ... | ...      | ...       | ...       |    ... |         ... | ...      | ...      |\n",
    "\n",
    "__Rankings DataFrame:__\n",
    "\n",
    "To scrape the athletes' rankings, we used selenium to navigate between years on `https://www.digitalrock.de/icc_calendar.php` and found the IFSC or UIAA world ranking pages that had tables with athletes, their rank for the year, their total points for the year, what world cups they competed in, and their scores those years. This dataset contains approximately 8,600 rows.\n",
    "\n",
    "|    id |   rank |   points | event   | gender   |   year |\n",
    "|------:|-------:|---------:|:--------|:---------|-------:|\n",
    "|  8372 |      1 |      300 | lead    | MEN      |   2019 |\n",
    "| 56609 |      2 |      256 | lead    | MEN      |   2019 |\n",
    "|  5089 |      3 |      206 | lead    | MEN      |   2019 |\n",
    "| 14023 |      4 |      195 | lead    | MEN      |   2019 |\n",
    "|  ... |      ... |      ... | ...    | ...      |   ... |\n",
    "\n",
    "__World Cup DataFrame:__\n",
    "\n",
    "Finally, we realized that those ranking pages didn't include all the athlete's world cup scores for the year, so we scraped the final data from `https://www.digitalrock.de/egroupware/ranking/sitemgr/digitalrock/eliste.html#!year={year}` for each year from 1991-2019 so we knew all of an athlete's world cup scores for that year. This dataset contains approximately 38,600 rows.\n",
    "\n",
    "|   athlete_id |   rank |   year | title                     | date          |  comp_id |   cat_id | type   |\n",
    "|-------------:|-------:|-------:|:--------------------------|:--------------|---------:|---------:|:-------|\n",
    "|          266 |      1 |   1991 | uiaa worldcup - wien 1991 | 26 April 1991 |       85 |        1 | lead   |\n",
    "|          849 |      2 |   1991 | uiaa worldcup - wien 1991 | 26 April 1991 |       85 |        1 | lead   |\n",
    "|          461 |      3 |   1991 | uiaa worldcup - wien 1991 | 26 April 1991 |       85 |        1 | lead   |\n",
    "|          973 |      4 |   1991 | uiaa worldcup - wien 1991 | 26 April 1991 |       85 |        1 | lead   |\n",
    "|          ... |      ... |   ... | ... | ... |       ... |        ... | ...   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Feature Engineering\n",
    "\n",
    "### Cleaning\n",
    "Our data was pretty clean to beging with, given that the website we used catered well to scraping with lots of clear tables and organization of data. We did however need to clean the following:\n",
    "1. Drop the old age column because we already have the `birth_year` column that conveys information in a more understandable way\n",
    "2. Setting `birth_year` to `NaN` for years that didn't make sense. Namely anything outside the range 1910-2017.\n",
    "3. Convert the `birth_year` column into a new `age` column that actually represents how old they were at that event. Drop the `birth_year` column as it is simply a different form of the new `age` column.\n",
    "4. Getting rid of units and converting height and weight into floats.\n",
    "5. Merging our athlete and ranking datasets to limit our models to only ranked climbers.\n",
    "6. Dropping duplicate columns during merging process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Athlete Data\n",
    "athlete_df = pd.read_csv('data/athlete_data.csv', index_col='id')\n",
    "clean_athletes = athlete_df.drop(columns='age')\n",
    "\n",
    "bad_ages_mask = ~clean_athletes['birth_year'].isin(np.arange(1990-80, 2020-3))\n",
    "bad_ages = clean_athletes[bad_ages_mask]['birth_year'].unique()\n",
    "clean_athletes.loc[clean_athletes['birth_year'].isin(bad_ages)] = np.nan\n",
    "\n",
    "clean_athletes['height'] = clean_athletes['height'].str.slice(0,-3).astype(float)\n",
    "clean_athletes['weight'] = clean_athletes['weight'].str.slice(0,-2).astype(float)\n",
    "\n",
    "# Merging the Athlete Data and the Ranking Data\n",
    "ranking_df = pd.read_csv('data/rankings.csv')\n",
    "ranked_athlete_ids = clean_rankings['ID'].unique()\n",
    "ranked_athletes = athletes.loc[athletes.index.isin(ranked_athlete_ids)]\n",
    "ranked_athletes['age'] = ranked_athletes['year'] - ranked_athletes['birth_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   id |   rank | last_name   | first_name   |   points | event   | gender   |   year | country   |   birth_year |   height |   weight |   age |\n",
    "|-----:|-------:|:------------|:-------------|---------:|:--------|:---------|-------:|:----------|-------------:|---------:|---------:|------:|\n",
    "| 8372 |      1 | Ondra       | Adam         |      300 | lead    | MEN      |   2019 | CZE       |         1993 |      185 |       67 |    26 |\n",
    "| 8372 |      2 | Ondra       | Adam         |      335 | boulder | MEN      |   2019 | CZE       |         1993 |      185 |       67 |    26 |\n",
    "| 8372 |     31 | Ondra       | Adam         |       55 | lead    | MEN      |   2018 | CZE       |         1993 |      185 |       67 |    25 |\n",
    "| 8372 |     28 | Ondra       | Adam         |       80 | lead    | MEN      |   2017 | CZE       |         1993 |      185 |       67 |    24 |\n",
    "| ... |     ... | ...       | ...         |       ... | ...    | ...      |   ... | ...       |         ... |      ... |       ... |    ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "There were a few key features that engineered on our data that we believed could tell us a lot about the athlete's climbing ability. For the purpose of simplicity, we limit the following data to the Mens lead climbing discipline, but note that the same process can be applied to the other sets of data.\n",
    "\n",
    "1. We first filter down to the rows with `event == 'lead'` and `gender == 'MEN'` to run our models on and remove the superfluous columns.\n",
    "2. We add a `career_len` feature that tells us how many years an athlete has been competing since their first year. This feature applies to each year of competition and looks back their first year to compute it.\n",
    "3. We add an `event_count` feature to get the amount of events an athlete went to that year. This is useful for calculating the \"lag\" columns later, but we eventually drop this in order to more easily predict further into the future.\n",
    "4. We add an `avg_points` feature to each row of the athlete's average points in the world cups that year.\n",
    "5. We generate \"lag\" columns. These columns are labeled as `t-x` where `x` is the number of years back that column represents. For example, `t-3` will hold the `avg_points` feature from 3 years before the given year. This allows us to train and test our data given that we know their performance for the previous $K$ years. For our purposes, we chose $K = 7$ because that seemed to adequately span most climber's careers, giving us enough data for most climbers, without overfitting or ending up with too many `NaN`s in most rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our data to \"MEN lead\" ranked athletes\n",
    "merged_ml = ranked_athletes[(ranked_athletes['event'] == 'lead') & (ranked_athletes['gender'] == 'MEN')].copy()\n",
    "merged_ml = merged_ml.drop(columns=['event', 'gender'])\n",
    "merged_ml = merged_ml.reset_index(drop=True)\n",
    "\n",
    "# Adding the career_len feature\n",
    "athlete_ids = merged_ml['id'].unique()\n",
    "experience = pd.DataFrame()\n",
    "experience_list = []\n",
    "for ID in athlete_ids:\n",
    "    athlete = merged_ml[merged_ml['id'] == ID]\n",
    "    # Get their first year\n",
    "    starting_year = athlete['year'].min()\n",
    "    # Calculate the career length\n",
    "    athlete_experience = athlete['year'] - starting_year\n",
    "    experience_list.append(athlete_experience)\n",
    "# Add the feature\n",
    "merged_ml['career_len'] = pd.concat(experience_list)\n",
    "\n",
    "# Add event_count feature\n",
    "men_lead_count_dfs = {}\n",
    "for year in range(2019, 1990, -1):\n",
    "    # Get the counts of events\n",
    "    year_df = merged_ml[merged_ml['year'] == year]\n",
    "    men_lead_count_dfs[year] = year_df['athlete_id'].value_counts()\n",
    "\n",
    "all_counts = []\n",
    "for i, row in merged_ml.iterrows():\n",
    "    count_df = men_lead_count_dfs[row['year']]\n",
    "    all_counts.append(count_df[row['id']])\n",
    "merged_ml['event_count'] = all_counts\n",
    "\n",
    "# Add avg_points feature\n",
    "merged_ml['avg_points'] = merged_ml['Points'] / merged_ml['event_count']\n",
    "\n",
    "# Add Lag Column Features\n",
    "top_athletes = merged_ml['id'].unique()\n",
    "K = 7 # Number of \"Lag Columns\" to generate\n",
    "col_names = [f't-{k+1}' for k in range(K)]\n",
    "lag_df = pd.DataFrame(columns=col_names)\n",
    "for i, ID in enumerate(top_athletes):\n",
    "    athlete = merged_ml[merged_ml['id'] == ID].copy()\n",
    "    # Shift the avg_points over to create the lag\n",
    "    lag_list = [athlete['avg_points'].shift(-(k+1)) for k in range(K)]\n",
    "    lags = pd.concat(lag_list, axis=1)\n",
    "    lags.columns = col_names\n",
    "    # Add these lag columns to all the rest of the lag columns\n",
    "    lag_df = pd.concat([lag_df, lags], axis=0)\n",
    "\n",
    "# Add the lag columns to the original data\n",
    "merged_ml = pd.concat([merged_ml, lag_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Cleaning\n",
    "\n",
    "We performed some small adjustments at the end of our cleaning and feature engineering, and before funning our models.\n",
    "\n",
    "__Imputing:__\n",
    "\n",
    "To impute our dataset and remove the `NaN`s, we did a few different things.\n",
    "1. For height, weight, and age, we noticed that climbers from the same country tended to be similar in age and body type, so we imputed those values using the mean from the climbers country.\n",
    "2. For the \"lag\" columns, we assumed that the climber had an avg_points of 0 in years where they didn't compete, so just replaced those `NaN` values with 0.\n",
    "\n",
    "__One-hot Encoding:__\n",
    "\n",
    "We one-hot encoded the country column so we can use it as a feature.\n",
    "\n",
    "__Train/Test Split:__\n",
    "\n",
    "We trained our models on the years 1990-2014 and then tested our model on the next 5 years (2015-2019). This allows us to test on a larger dataset which lowers the variance of our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `impute` is a longer function that compltes the imputing steps shown above\n",
    "df = impute(merged_ml)\n",
    "# Drop string and unimportant columns\n",
    "df = df.drop(['id', 'last_name', 'first_name', 'points', 'rank', 'event_count'], axis=1)\n",
    "# One-hot encode the `country` column\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Get our test data\n",
    "test = df[df['year'] >= 2015]\n",
    "X_test = test.drop(['avg_points', 'year'], axis=1)\n",
    "y_test = test['avg_points']\n",
    "\n",
    "# Similarly, get our training data\n",
    "train = df[df['year'] <= 2014]\n",
    "X_train = train.drop(['avg_points', 'year'], axis=1)\n",
    "y_train = train['avg_points']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final Dataset:__\n",
    "\n",
    "| height   | weight   | age | career\\_len | avg\\_points | t\\-1     | t\\-2    | t\\-\\.\\.\\. | country\\_CZE | country\\_ESP | country\\_\\.\\.\\. |\n",
    "|----------|----------|-----|-------------|-------------|----------|---------|-----------|--------------|--------------|-----------------|\n",
    "| 185      | 67       | 26  | 10          | 100         | 55       | 80      | \\.\\.\\.    | 1            | 0            | \\.\\.\\.          |\n",
    "| 179\\.048 | 62\\.9524 | 29  | 1           | 42\\.6667    | 43       | 0       | \\.\\.\\.    | 0            | 1            | \\.\\.\\.          |\n",
    "| 179\\.048 | 62\\.9524 | 20  | 16          | 34\\.3333    | 25\\.6667 | 63\\.5   | \\.\\.\\.    | 0            | 0            | \\.\\.\\.          |\n",
    "| 179\\.048 | 62\\.9524 | 19  | 3           | 39          | 12       | 6\\.5    | \\.\\.\\.    | 0            | 0            | \\.\\.\\.          |\n",
    "| 185      | 67       | 25  | 9           | 31\\.6667    | 66\\.5714 | 51\\.625 | \\.\\.\\.    | 0            | 0            | \\.\\.\\.          |\n",
    "| ...      | ...       | ...  | ...           | ...    | ... | ... | \\.\\.\\.    | ...            | ...            | \\.\\.\\.          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms and In-depth Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models\n",
    "To run our models and display our accuracy, we used the following code for each model. As a comparison, we used a naive baseline model which just predicted that athlete's scores would just be the same from the previous year.\n",
    "\n",
    "Our metric for measuring the performance of our models was the Mean Squared Error (MSE) and the Mean Absolute Error (MAE). We choose to use the MSE because the MSE punishes large errors but is not so harsh on small errors. This was perfect for our purposes because we are attempting to regress near to our data without overfitting. The MAE was also included but not for optimization. Instead it was used only for interpretability. We show both the Mean Squared Error (MSE) and the Mean Absolute Error (MAE) for each of the models we ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_model(model, param_grid, model_name):\n",
    "    # Perform the Grid Search using all available cores\n",
    "    grid = GridSearchCV(model, param_grid, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
    "    # Predict based on our test data\n",
    "    pred = grid.predict(X_test)\n",
    "    # Print out the MSE of our predictions and the best params found with the grid search\n",
    "    print(\"MSE Value:\", mean_squared_error(pred, y_test))\n",
    "    print(\"MAE Value:\", mean_absolute_error(pred, y_test))\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "    # Make a scatter plot like the one shown below\n",
    "    plt.scatter(range(len(pred)), pred, label='Prediction')\n",
    "    plt.scatter(range(len(pred)), y_test, label='Actual AVG Points')\n",
    "    plt.title(f'{model_name} - GridSearchCV')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Make the bar plot shown below\n",
    "    plt.bar(range(len(pred)), np.abs(pred - y_test))\n",
    "    plt.title(f'{model_name} - Absolute Errors')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our results:\n",
    "\n",
    "| Model Name | Best Parameters | Mean Squared Error | Mean Absolute Error |\n",
    "| :-- | :-- | --: | --: |\n",
    "| Baseline Model | N/A | 195.27 | 9.65 |\n",
    "| Linear Regression | Default | 155.18 | 9.21 |\n",
    "| Linear Regression (Lasso Regularization) | `{'alpha': 1.0}` | 159.64 | 9.32 |\n",
    "| Linear Regression (Ridge Regularization) | `{'alpha': 1000.0}` | 155.5 | 9.08 |\n",
    "| Linear Regression (Elastic Net Regularization) | `{'alpha': 0.1, 'l1_ratio': 1.0}` | 159.65 | 9.32 |\n",
    "| Decision Tree Regressor | `{'max_depth': 10, 'max_leaf_nodes': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}` | 168.47 | 9.54 |\n",
    "| Random Forest Regressor | `{'max_depth': 5, 'max_features': 'auto', 'min_samples_leaf': 10, 'n_estimators': 50}` | 169.76 | 9.66 |\n",
    "| Gradient Boosting Regressor | `{'learning_rate': 0.1, 'max_depth': 2, 'max_features': 'auto', 'min_samples_leaf': 10, 'n_estimators': 50}` | 170.11 | 9.65 |\n",
    "| XGBoost | `{'alpha': 0, 'eta': 0.1, 'gamma': 0, 'lambda': 0, 'max_depth': 1}` | 234.38 | 8.66 |\n",
    "\n",
    "Our best model by the MSE was Linear Regression. Despite the high MSE of XGBoost, however, it had a surprisingly low MAE, leading us to believe that it got many close predictions with a few predictions very far away, as MSE heavily penalizes being farther away from the correct value.\n",
    "\n",
    "We also pulled the feature importances from our Random Forest Regressor, which performed better than our baseline. We show the 10 most important features here:\n",
    "\n",
    "| Feature | Importance |\n",
    "| --- | --: |\n",
    "| t-1          |   0.828737 |\n",
    "| t-2          |   0.087144 |\n",
    "| t-3          |   0.019983 |\n",
    "| age          |   0.013638 |\n",
    "| weight       |   0.010308 |\n",
    "| t-4          |   0.009976 |\n",
    "| country_FRA  |   0.007835 |\n",
    "| career_len   |   0.007252 |\n",
    "| height       |   0.006860 |\n",
    "| t-5          |   0.001890 |\n",
    "\n",
    "As we thought, our lag columns were more important than most features. Specifically, the previous year's score proved to be a very important indicator of their performance the next year.\n",
    "\n",
    "__Womens Lead:__\n",
    "\n",
    "We also ran our code on the womens lead discipline with our Linear Regression model and found similar results with an MSE of 162.35 and and MAE of 9.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting 5 Years into the Future\n",
    "Returning to our original research question, we hoped to be able to accurately predict the best climbers 5 years into the future. To do this, we trained our model as before on data from years 1990-2015. Then we predicted the next year's averages. Then we shifted our lag columns down, letting `avg_points` be the predicted values, `t-1` the previous year's values, and so forth. Then, we predict the next year's data and so on until we reach 2019. Here are our predictions, and their accuracy. We also include a figure showing the accuracy (MSE) for each of the years up to and including 2019.\n",
    "\n",
    "| First Name | Last Name | Predicted Average | Actual Average | Difference in Averages | Predicted Rank | Actual Rank |\n",
    "|------------|-----------|-------------------|----------------|------------------------|----------------|-------------|\n",
    "| Jakob      | Schubert  | 63\\.0651          | 44             | 19\\.0651               | 1              | 14          |\n",
    "| Adam       | Ondra     | 57\\.0388          | 100            | 42\\.9612               | 2              | 1           |\n",
    "| Domen      | Skofic    | 44\\.3178          | 36\\.6667       | 7\\.65118               | 3              | 22          |\n",
    "| Stefano    | Ghisolfi  | 43\\.8053          | 31\\.6667       | 12\\.1386               | 4              | 5           |\n",
    "| Sean       | McColl    | 42\\.5345          | 34\\.3333       | 8\\.20118               | 5              | 3           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models we chose not to use\n",
    "Because our target value was continuous, we only used regression methods, rather than using classification methods. We also largely avoided unsupervised methods. During our exploration phase, we ran t-SNE, but were unable to observe any meaningful clusters. We determined that the problem we were most interested in solving was the supervised problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Implications\n",
    "With our results, it is important to make several ethical considerations. First, our model includes country of origin as a feature. Although certain countries were identified as important features, we are in no way suggesting that an individual athlete should be assumed to be a good/bad climber based on the country they are from.\n",
    "\n",
    "Second, if our model became widely used, it could produce a feedback loop between sponsorship and athlete performance. The industry of professional climbing is not yet at a point where successful athletes make a lot of money, therefore sponsorship may make a significant difference on how much time a professional athlete is able to devote to training, and what gear/resources are available to them . If sponsors could forecast with high reliability who is likely to perform best next year, they would want to sponsor those athletes. Generous sponsorship would allow those athletes to train more and perform better the year following, and consequently receive more sponsorships. This would negatively affect those athletes who may have received sponsorship without our model being considered. Our model will currently not be made available publicly, and if it were eventually, we would ensure that it was maintainable, and could be deactivated if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In conclusion, we were successful in outperforming the baseline model in predicting the average points per event of athletes. Our model improved the MSE from 195.7 to 155.5 (-40.2). Furthermore, our analysis showed that the biggest predictors of an athletes’ performance are their performance in the previous three years, along with their age and weight. However, as we anticipated, our model did not perform as well when predicting the ranks of athletes. This is a result of athletes skipping events. While our model may not be able to predict the winners of the IFSC World Cup in future years, we feel that it does a decent job of predicting which climbers will perform the best on average. That is to say that if each athlete attended all the events, we feel our model would be able to predict World Cup winners with much higher confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
